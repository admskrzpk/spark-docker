@startuml
!include https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml

' caption Context Diagram for Horrendous Hats (HH) Website

LAYOUT_WITH_LEGEND()

Person_Ext(user1, "User", "A internet user")
Person_Ext(user2, "User", "A internet user")
Person_Ext(user3, "User", "A internet user")
System_Boundary(highLoad, "High Load Platform") {
Container(slb, "Smart Load Balancer", "Takes all the requests from the internets")
Container(bidder, "Bidder", "Bidds in the Exchenge")
Container(serving, "Serving", "Serving of Ad's")
}


SystemDb(kafka, "KAFKA","Events bus")
@enduml

= Dockerized Spark App Example
================================================


== Project description
****
The goal of this project is to test how to dockerize simple Spark app using sbt docker plugin. +

App itself just loading and displaying files from a directory.
****

== Tools

https://spark.apache.org/[Apache Spark] +
https://github.com/marcuslonnberg/sbt-docker[sbt docker]

== How to build project & run
* Clone and import project to Idea Intelij. Other IDE's not tested.

* Add path to folder as a command line argument  to the app which contains files (or hardcoded one) to display

* Build project using sbt Docker

[source,text]
----
sbt docker
----
* In your command line terminal go to working directory and pass command. Example:

[source,text]
----
docker run -v /home/admskrzpk/spark-3.2.1-bin-hadoop3.2:/tmp/adam/ adam/spark-docker /tmp/adam/README.md
----

== Troubleshooting
Make sure that your project does not contain outdated project files using:
[source, text]
----
sbt clean
----

* Make sure that correct dependencies are added to your build.sbt file:

[source,text]
----
libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.2.1" % "provided"
----

* Make sure that sbt plugin is correctly placed in project files.

* Make sure that your Scala and Spark version are compatible. 
